---
title: "HW6"
author: "Xinyu Shen xs2384"
date: "2019/11/22"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(purrr)
```


## Problem 1


#### Import and clean the data


```{r, warning=FALSE, message=FALSE}

df = read_csv("./data/birthweight.csv") %>% janitor::clean_names() %>% mutate(
  babysex = as.factor(babysex),
  frace = as.factor(frace),
  malform = as.factor(malform),
  mrace = as.factor(mrace)
)

# check missing value
nrow(df[is.na(df),])

```

From above, we can see that there is no missing value in the dataset. 



#### Model Selection


I use the stepwise to select model

```{r,warning=FALSE, message=FALSE}

my_model = step(lm(bwt~., data = df), direction = "backward")


```


The model selected by stepwise is:

```{r,warning=FALSE, message=FALSE}
my_model %>% summary()
```


The stepwise select the best model with significant coefficients. The F test shows that the p-value for selected model is far less than 0.05. 


#### Plot of residuals vs fitted value

```{r,warning=FALSE, message=FALSE}
df %>% add_predictions(my_model) %>% add_residuals(my_model) %>% ggplot(aes(x = pred, y = resid)) + geom_point() + geom_smooth(se = F) + theme_bw() + labs(x = "Fitted Value", y = 'Residuals', title = "Residuals vs Fitted value") + theme(plot.title = element_text(hjust = 0.5))
```

From the plot above, we can see that the residuals are normally distributed when fitted value is from 2000 to 4000. 


#### Comparing with two models

```{r,warning=FALSE, message=FALSE}

set.seed(1)

cv_df = crossv_mc(df, 100)

cv_df = cv_df %>% 
  mutate(train = map(train, as_tibble),
    test = map(test, as_tibble)) 

cv_df = cv_df %>% mutate(
    my_model = map(train, ~lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data =.x)),
    model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data =.x)),
    model_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))

  ) %>% 
  mutate(
      rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
      rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
      rmse_model_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y))) 

cv_df %>% select(starts_with("rmse")) %>% 
  pivot_longer(everything() ,
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin(alpha = 0.5) + theme_bw()
```


From the violin plot above, we can see that my choice of model has the lowest rmse compared with other two models, which means my model has least predicted error. 



## Problem 2

#### Import the data

```{r,warning=FALSE, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())


```


#### Calculate the R^2 and log(beta0 + beta1)


```{r,warning=FALSE, message=FALSE}
l = function(x){
  lo = log(x[[2]][1] * x[[2]][2])
  lo
}

r = function(x){
  r2 = x[[2]]
  r2
}

set.seed(1)

boots_trap = weather_df %>% bootstrap(n = 5000) %>% mutate(
  models = map(strap, ~lm(tmax ~ tmin, data = .x)),
  r_2 = map(models, broom::glance),
  log = map(models, broom::tidy),
  r_2 = as.numeric(map(r_2, r)),
  log = as.numeric(map(log, l))
  ) %>% select(-strap, -models) %>% rename("r_square" = r_2)


```


#### Plot the distribution of estimates


```{r,warning=FALSE, message=FALSE}
boots_trap %>% ggplot(aes(x = r_square)) + geom_density(fill = "red", alpha = 0.6) + labs(title = "Distribution of R^2") + theme_bw() + theme(plot.title = element_text(hjust = 0.5))
```


From the plot above, we can see that r-squared score for each strap is normally distributed with a little left skewness, which means that there are some extremely high r-squared score, the outliers, for some of the straps. 



```{r,warning=FALSE, message=FALSE}
boots_trap %>% ggplot(aes(x = log)) + geom_density(fill = "green", alpha = 0.6) + labs(title = "Distribution of log(beta0 * beta1)") + theme_bw() + theme(plot.title = element_text(hjust = 0.5))
```


From the above plot, we can see that the log(beta1 * beta2) estimates are normally distributed through the straps. 


#### 95% confidence interval with quantile 0.025, 0.975


##### R-square

```{r,warning=FALSE, message=FALSE}

c(quantile(boots_trap$r_square, probs = 0.025), quantile(boots_trap$r_square, probs = 0.975))

```

The 95% confidence interval for r-square with quantile 0.025 and 0.975 is (`r quantile(boots_trap$r_square, probs = 0.025)`, `r quantile(boots_trap$r_square, probs = 0.975)`)


##### log(beta0 * beta1)

```{r,warning=FALSE, message=FALSE}
c(quantile(boots_trap$log, probs = 0.025), quantile(boots_trap$log, probs = 0.975))
```

The 95% confidence interval for log(beta0 * beta1) with quantile 0.025 and 0.975 is (`r quantile(boots_trap$log, probs = 0.025)`, `r quantile(boots_trap$log, probs = 0.975)`)
